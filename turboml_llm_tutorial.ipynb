{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhkaul/Assignment_1_Empowered_Coder/blob/master/turboml_llm_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TurboML LLM Tutorial\n",
        "TurboML can spin up LLM servers with an OpenAI-compatible API. We currently support models\n",
        "in the GGUF format, but also support non-GGUF models that can be converted to GGUF. In the latter\n",
        "case you get to decide the quantization type you want to use."
      ],
      "metadata": {
        "id": "7k9U2jhVAhHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the environment and install TurboML's SDK.\n",
        "We use `turboml-installer` to set up the environment for TurboML's SDK."
      ],
      "metadata": {
        "id": "43Fbi9beAhHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q turboml-installer\n",
        "import turboml_installer ; turboml_installer.install_on_colab()"
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK8W5h5VAhHV",
        "outputId": "26c4d778-d2e3-436e-f19b-02881a1c0439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüì¶ Installing...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:43\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel should now be restarted with TurboML's SDK installed."
      ],
      "metadata": {
        "id": "LClbQLQwAhHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to your TurboML instance\n",
        "\n",
        "Note that you can copy and replace this snippet with one from your TurboML homepage."
      ],
      "metadata": {
        "id": "QibB3c6-AhHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import turboml as tb\n",
        "\n",
        "tb.init(\n",
        "  backend_url=\"https://screeching-dolphin.api.turboml.online\",\n",
        "  api_key=\"tb_iVKKijh8TKeezNjButxCsCHqdYi8HreO_7e07ce66\"\n",
        ")"
      ],
      "execution_count": 3,
      "metadata": {
        "id": "eaC7uKnVAhHX"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LlamaServerRequest = tb.llm.LlamaServerRequest\n",
        "HuggingFaceSpec = LlamaServerRequest.HuggingFaceSpec\n",
        "ServerParams = LlamaServerRequest.ServerParams"
      ],
      "execution_count": 4,
      "metadata": {
        "id": "Pzsnvj0KAhHX"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose a model\n",
        "Let's use a Llama 3.2 quant already in the GGUF format."
      ],
      "metadata": {
        "id": "wKbBfdUzAhHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_spec = HuggingFaceSpec(\n",
        "    hf_repo_id=\"Mozilla/llava-v1.5-7b-llamafile\",\n",
        "    select_gguf_file=\"llava-v1.5-7b-Q4_K.gguf\",\n",
        ")"
      ],
      "execution_count": 7,
      "metadata": {
        "id": "IA8ZHreMAhHY"
      },
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spawn a server\n",
        "On spawning a server, you get a `server_id` to reference it later as well as `server_relative_url` you can\n",
        "use to reach it. This method is synchronous, so it can take a while to yield as we retrieve (and convert) your model."
      ],
      "metadata": {
        "id": "dzdaDzNAAhHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = tb.llm.spawn_llm_server(\n",
        "    LlamaServerRequest(\n",
        "        source_type=LlamaServerRequest.SourceType.HUGGINGFACE,\n",
        "        hf_spec=hf_spec,\n",
        "        server_params=ServerParams(\n",
        "            threads=-1,\n",
        "            seed=-1,\n",
        "            context_size=0,\n",
        "            flash_attention=False,\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "response"
      ],
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NRpVqivAhHZ",
        "outputId": "7ea104b8-cc2c-4b6f-8d5a-7051738cf1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:turboml.llm:[hf-acquisition] Status: in_progress, Progress: Downloading model from HF...\n",
            "INFO:turboml.llm:[hf-acquisition] Status: completed, Progress: Completed successfully.\n",
            "INFO:turboml.llm:[hf-acquisition] Acquisition Done, gguf_id = Mozilla$$llava-v1.5-7b-llamafile$$llava-v15-7b-Q4_Kgguf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaServerResponse(server_id='Mozilla$$llava-v1.5-7b-llamafile$$llava-v15-7b-Q4_Kgguf.1939615351', server_relative_url='/openai/Mozilla$$llava-v1.5-7b-llamafile$$llava-v15-7b-Q4_Kgguf.1939615351/api/v1')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "server_id = response.server_id"
      ],
      "execution_count": 9,
      "metadata": {
        "id": "gkzvQ45lAhHZ"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image, Audio\n",
        "import cv2  # We're using OpenCV to read video, to install !pip install opencv-python\n",
        "import base64\n",
        "import time\n",
        "# from openai import OpenAI\n",
        "import os\n",
        "import requests\n",
        "import datetime"
      ],
      "metadata": {
        "id": "i_AMNFD6Y8U4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = cv2.VideoCapture(\"/content/Video-642_480.mov\")\n",
        "frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# calculate duration of the video\n",
        "seconds = round(frames / fps)\n",
        "video_time = datetime.timedelta(seconds=seconds)\n",
        "print(f\"duration in seconds: {seconds}\")\n",
        "print(f\"video time: {video_time}\")"
      ],
      "metadata": {
        "id": "fYj-uvbNZgGY",
        "outputId": "4d58775f-ed9d-4695-ace3-8179f943904e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "duration in seconds: 23\n",
            "video time: 0:00:23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base64Frames = []\n",
        "while video.isOpened():\n",
        "    success, frame = video.read()\n",
        "    if not success:\n",
        "        break\n",
        "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
        "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
        "\n",
        "video.release()\n",
        "print(len(base64Frames), \"frames read.\")"
      ],
      "metadata": {
        "id": "a8j_ePLLZgEl",
        "outputId": "c9d2e809-2eec-4935-fffb-67ffe0160d36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "688 frames read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZfMj6IPZgCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44mRG8GiZf_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFLU7AwBZf39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OljiaIHWZfrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interacting with the LLM\n",
        "\n",
        "Our LLM is exposed with an OpenAI-compatible API, so we can use the OpenAI SDK, or any\n",
        "other tool compatible tool to use it."
      ],
      "metadata": {
        "id": "63xVQHwhAhHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "base_url = tb.common.env.CONFIG.TURBOML_BACKEND_SERVER_ADDRESS\n",
        "server_url = f\"{base_url}/{response.server_relative_url}\"\n",
        "\n",
        "client = OpenAI(base_url=server_url, api_key=\"-\")\n",
        "\n",
        "prompt = \"Describe whats happening in this instagram reel, also explain how we can improve this reel\"\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\":[\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "                {\"type\": \"image\", \"image\": base64Frames[0]},\n",
        "                {\"type\": \"image\", \"image\": base64Frames[100]},\n",
        "                {\"type\": \"image\", \"image\": base64Frames[500]},\n",
        "                {\"type\": \"image\", \"image\": base64Frames[600]},\n",
        "            ]\n",
        "\n",
        "        },\n",
        "    ],\n",
        "    model=\"-\",\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6wZ4EVmAhHZ",
        "outputId": "5da0ef14-f641-4feb-c7a7-4a8f409bd4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://screeching-dolphin.api.turboml.online//openai/Mozilla$$llava-v1.5-7b-llamafile$$llava-v15-7b-Q4_Kgguf.1939615351/api/v1/chat/completions \"HTTP/1.1 504 Gateway Time-out\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.441204 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxwJoBwFAhHZ",
        "outputId": "095ddf03-174f-450d-cbe7-8e4b8676a6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.61.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai) (1.9.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/site-packages (from openai) (2.10.6)\n",
            "Collecting sniffio (from openai)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Downloading openai-1.61.1-py3-none-any.whl (463 kB)\n",
            "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: sniffio, jiter, h11, httpcore, anyio, httpx, openai\n",
            "Successfully installed anyio-4.8.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.61.1 sniffio-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = (\n",
        "    client.embeddings.create(input=[\"Hello there how are you doing today?\"], model=\"-\")\n",
        "    .data[0]\n",
        "    .embedding\n",
        ")\n",
        "len(embeddings), embeddings[:5]"
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bXcvpsArAhHZ",
        "outputId": "15c2a8b0-aa52-433e-f0c2-ec666c6e3518"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ccdd8fd84d9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings = (\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Hello there how are you doing today?\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop the server"
      ],
      "metadata": {
        "id": "GiCT6DviAhHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.llm.stop_llm_server(server_id)"
      ],
      "execution_count": null,
      "metadata": {
        "id": "N9qv_NhEAhHa"
      },
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}